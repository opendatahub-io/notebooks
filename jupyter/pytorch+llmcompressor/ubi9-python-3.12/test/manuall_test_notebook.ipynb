{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2bae8c1",
   "metadata": {},
   "source": [
    "These llmcompressor notebooks need:\n",
    "- CUDA GPU enabled\n",
    "- Container Size: Large\n",
    "- Storage at least 50Gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec774a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "from llmcompressor import oneshot\n",
    "import os\n",
    "\n",
    "# Load model\n",
    "model_stub = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model_name = model_stub.split(\"/\")[-1]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_stub,\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_stub)\n",
    "\n",
    "# Configure the quantization algorithm and scheme\n",
    "recipe = QuantizationModifier(\n",
    "    targets=\"Linear\",\n",
    "    scheme=\"FP8_DYNAMIC\",\n",
    "    ignore=[\"lm_head\"],\n",
    ")\n",
    "\n",
    "# Apply quantization\n",
    "oneshot(\n",
    "    model=model,\n",
    "    recipe=recipe,\n",
    ")\n",
    "\n",
    "# Save to disk in compressed-tensors format\n",
    "save_path = model_name + \"-FP8-dynamic\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model and tokenizer saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848efc9d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "from llmcompressor.utils import dispatch_for_generation\n",
    "\n",
    "# NOTE: Requires a minimum of transformers 4.57.0\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2-1.5B\"\n",
    "\n",
    "# Load model.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "recipe = QuantizationModifier(\n",
    "    targets=[\"Linear\"],\n",
    "    scheme=\"FP8_DYNAMIC\",\n",
    "    ignore=[\n",
    "        \"lm_head\",\n",
    "        \"re:.*mlp.gate$\",\n",
    "        \"re:.*mlp.shared_expert_gate$\",\n",
    "        \"re:.*linear_attn.*\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply quantization.\n",
    "oneshot(model=model, recipe=recipe)\n",
    "\n",
    "# Confirm generations of the quantized model look sane.\n",
    "print(\"========== SAMPLE GENERATION ==============\")\n",
    "dispatch_for_generation(model)\n",
    "input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\n",
    "    model.device\n",
    ")\n",
    "output = model.generate(input_ids, max_new_tokens=20)\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(\"==========================================\")\n",
    "\n",
    "# Save to disk in compressed-tensors format.\n",
    "SAVE_DIR = MODEL_ID.rstrip(\"/\").split(\"/\")[-1] + \"-FP8-Dynamic\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
