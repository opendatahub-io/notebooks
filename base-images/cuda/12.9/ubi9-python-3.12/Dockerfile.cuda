ARG TARGETARCH

FROM registry.access.redhat.com/ubi9/ubi-minimal:latest AS buildscripts
COPY base-images/utils/aipcc.sh /mnt/aipcc.sh
COPY base-images/utils/fix-permissions /mnt/usr/bin/

####################
# base             #
####################
FROM registry.access.redhat.com/ubi9/python-312:latest AS base

USER 0
ARG TARGETARCH
ENV TARGETARCH=${TARGETARCH}
ENV PYTHON=python3.12
ENV VIRTUAL_ENV=/opt/app-root/

# OpenShift s2i / Cloud Native Buildpack user
ENV CNB_USER_ID=1001 \
    CNB_GROUP_ID=0

# Custom fix-permissions script that extends sclorg version with chown to UID 1001
COPY --from=buildscripts /mnt/usr/bin/ /usr/bin/

RUN \
--mount=from=buildscripts,source=/mnt,target=/mnt \
--mount=type=cache,sharing=locked,id=dnf-ubi9,target=/var/cache/dnf \
/bin/bash <<'EOF'
set -Eeuxo pipefail
/mnt/aipcc.sh
fix-permissions ${APP_ROOT} -P
EOF

USER 1001
WORKDIR /opt/app-root/bin

####################
# cuda-base        #
####################
FROM base AS cuda-base-amd64
ENV NVARCH=x86_64

FROM base AS cuda-base-arm64
ENV NVARCH=sbsa

FROM cuda-base-${TARGETARCH} AS cuda-base

ARG TARGETARCH

# Install CUDA base from:
# https://gitlab.com/nvidia/container-images/cuda/-/blob/master/dist/12.9.1/ubi9/base/Dockerfile
USER 0
WORKDIR /opt/app-root/bin

ENV NVIDIA_REQUIRE_CUDA="cuda>=12.9 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566 brand=unknown,driver>=570,driver<571 brand=grid,driver>=570,driver<571 brand=tesla,driver>=570,driver<571 brand=nvidia,driver>=570,driver<571 brand=quadro,driver>=570,driver<571 brand=quadrortx,driver>=570,driver<571 brand=nvidiartx,driver>=570,driver<571 brand=vapps,driver>=570,driver<571 brand=vpc,driver>=570,driver<571 brand=vcs,driver>=570,driver<571 brand=vws,driver>=570,driver<571 brand=cloudgaming,driver>=570,driver<571"
ENV NV_CUDA_CUDART_VERSION=12.9.79-1

ARG CUDA_REPOS=base-images/cuda/12.9/cuda-repos
COPY ${CUDA_REPOS}/cuda.repo-${TARGETARCH} /etc/yum.repos.d/cuda.repo
COPY ${CUDA_REPOS}/NGC-DL-CONTAINER-LICENSE /

RUN NVIDIA_GPGKEY_SUM=d0664fbbdb8c32356d45de36c5984617217b2d0bef41b93ccecd326ba3b80c87 && \
    curl -fsSL https://developer.download.nvidia.com/compute/cuda/repos/rhel9/${NVARCH}/D42D0685.pub | sed '/^Version/d' > /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA && \
    echo "$NVIDIA_GPGKEY_SUM  /etc/pki/rpm-gpg/RPM-GPG-KEY-NVIDIA" | sha256sum -c --strict -

ENV CUDA_VERSION=12.9.1

# For libraries in the cuda-compat-* package: https://docs.nvidia.com/cuda/eula/index.html#attachment-a
RUN dnf upgrade -y && dnf install -y \
    cuda-cudart-12-9-${NV_CUDA_CUDART_VERSION} \
    cuda-compat-12-9 \
    && dnf clean all \
    && rm -rf /var/cache/yum/*

# nvidia-docker 1.0
RUN echo "/usr/local/cuda/lib64" >> /etc/ld.so.conf.d/nvidia.conf

ENV PATH=/usr/local/cuda/bin:${PATH}
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64

# nvidia-container-runtime
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility,video

# Disable JIT caching for reproducibility
# https://developer.nvidia.com/blog/cuda-pro-tip-understand-fat-binaries-jit-caching/
ENV CUDA_CACHE_DISABLE=1

# Load kernels on demand for faster startup
# With CUDA_MODULE_LOADING=LAZY:
# - A simple import torch; torch.cuda.is_available() won't catch all issues
# Eager loading for tests - Override during test:
#     CUDA_MODULE_LOADING=EAGER python -c "import torch; ..."
#ENV CUDA_MODULE_LOADING=LAZY

# Install CUDA runtime from:
# https://gitlab.com/nvidia/container-images/cuda/-/blob/master/dist/12.9.1/ubi9/runtime/Dockerfile
ENV NV_CUDA_LIB_VERSION=12.9.1-1
ENV NV_NVTX_VERSION=12.9.79-1
ENV NV_LIBNPP_VERSION=12.4.1.87-1
ENV NV_LIBNPP_PACKAGE=libnpp-12-9-${NV_LIBNPP_VERSION}
ENV NV_LIBCUBLAS_VERSION=12.9.1.4-1
ENV NV_LIBNCCL_PACKAGE_NAME=libnccl
ENV NV_LIBNCCL_PACKAGE_VERSION=2.27.3-1
ENV NV_LIBNCCL_VERSION=2.27.3
ENV NCCL_VERSION=2.27.3
ENV NV_LIBNCCL_PACKAGE=${NV_LIBNCCL_PACKAGE_NAME}-${NV_LIBNCCL_PACKAGE_VERSION}+cuda12.9

RUN dnf install -y \
    cuda-libraries-12-9-${NV_CUDA_LIB_VERSION} \
    cuda-nvtx-12-9-${NV_NVTX_VERSION} \
    ${NV_LIBNPP_PACKAGE} \
    libcublas-12-9-${NV_LIBCUBLAS_VERSION} \
    ${NV_LIBNCCL_PACKAGE} \
    && dnf clean all \
    && rm -rf /var/cache/yum/*

# Install devel tools
RUN dnf install -y \
    make \
    findutils \
    && dnf clean all \
    && rm -rf /var/cache/yum/*

# Install CUDA cudnn9 from:
# https://gitlab.com/nvidia/container-images/cuda/-/blob/master/dist/12.9.1/ubi9/runtime/cudnn/Dockerfile
ENV NV_CUDNN_VERSION=9.14.0.64-1
ENV NV_CUDNN_PACKAGE libcudnn9-cuda-12-${NV_CUDNN_VERSION}

LABEL com.nvidia.cudnn.version="${NV_CUDNN_VERSION}"

RUN dnf install -y \
    ${NV_CUDNN_PACKAGE} \
    && dnf clean all \
    && rm -rf /var/cache/yum/*

# Set this flag so that libraries can find the location of CUDA
ENV XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda

# Install additional CUDA libraries needed by PyTorch
# cuda-cupti: CUDA Profiling Tools Interface (libcupti.so)
# libcusparselt0: cuSPARSELt for structured sparsity (libcusparseLt.so)
# libcudss0-cuda-12: cuDSS direct sparse solver (libcudss.so)
RUN dnf -y install \
    cuda-cupti-12-9 \
    libcusparselt0 \
    libcudss0-cuda-12 \
    && dnf clean all \
    && rm -rf /var/cache/yum/*

# Runtime dependencies for PyTorch packages (libmpi_cxx.so, libqhull_r.so, libsnappy.so)
# These packages (openmpi, libqhull_r, snappy) are not available in UBI9 repos without
# RHEL entitlements. Adding CentOS Stream 9 repos would work but mixing distro repos
# is not a good practice to promote. The c9s variant of this image has these installed.

### BEGIN AIPCC pip and uv config files
ARG INDEX_URL
COPY --chmod=664 --chown=1001:0 base-images/utils/pip.conf.in /opt/app-root/pip.conf
COPY --chmod=664 --chown=1001:0 base-images/utils/uv.toml.in /opt/app-root/uv.toml
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
if [ -z "${INDEX_URL}" ]; then
  echo "ERROR: INDEX_URL build arg is required" >&2
  exit 1
fi
sed -i "s|@INDEX_URL@|${INDEX_URL}|g" /opt/app-root/pip.conf
sed -i "s|@INDEX_URL@|${INDEX_URL}|g" /opt/app-root/uv.toml
EOF

# Python and virtual env settings
ENV VIRTUAL_ENV=${APP_ROOT} \
    PIP_CONFIG_FILE=/opt/app-root/pip.conf \
    UV_CONFIG_FILE=/opt/app-root/uv.toml \
    PIP_NO_CACHE_DIR=off \
    UV_NO_CACHE=true \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONIOENCODING=utf-8 \
    LANG=en_US.UTF-8 \
    LC_ALL=en_US.UTF-8 \
    PS1="(app-root) \w\$ "
### END AIPCC pip and uv config files

# Restore user workspace
USER 1001
WORKDIR /opt/app-root/src
