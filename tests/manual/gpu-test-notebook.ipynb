{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06fb33f2-64d3-4555-97ae-1fbd4d35bcf3",
   "metadata": {},
   "source": [
    "# GPU Test Notebook\n",
    "\n",
    "This notebook aims to provide a very basic testing perspective on Jupyter notebooks with GPU support, in such way that:\n",
    "\n",
    "1. Notebook environment setup\n",
    "2. Verify the installed Python version\n",
    "3. Find the version of the installed TensorFlow or PyTorch packages\n",
    "4. Find the GPU on the devices list\n",
    "5. Check if GPU drivers (nvidia-smi or rocm-smi) loads properly\n",
    "6. CUDA/ROCm drivers are installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311fd022-69d3-4b0b-a60a-6605bf5800af",
   "metadata": {},
   "source": [
    "## 1. Notebook environment setup\n",
    "\n",
    "To provide some easier functions across the notebook, a setup will make some system calls more readable and easier to understand or execute:\n",
    "\n",
    "- import standard packages that will be used on this notebook, i.e., `os`, `sys`, etc;\n",
    "- check if this environment is running a TensorFlow or PyTorch image / environment;\n",
    "- check if this enviromment contains a CUDA or ROCm GPU;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c79f98af-66a6-4d73-8d8c-a1079a07510b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Initialize framework and GPU type\n",
    "framework = \"NONE\"\n",
    "gpu_type = \"CPU\"\n",
    "\n",
    "def import_pytorch():\n",
    "    global framework, gpu_type\n",
    "    import torch\n",
    "    framework = \"PYTORCH\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # Check if it's actually ROCm by looking at device name\n",
    "        try:\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "            # Check for AMD/ROCm indicators in device name\n",
    "            if any(keyword in device_name.upper() for keyword in ['AMD', 'RADEON', 'INSTINCT', 'MI300', 'MI250', 'MI100']):\n",
    "                gpu_type = \"ROCM\"\n",
    "            else:\n",
    "                gpu_type = \"CUDA\"\n",
    "        except:\n",
    "            # Fallback: check if ROCm version exists\n",
    "            if hasattr(torch.version, 'hip') and torch.version.hip is not None:\n",
    "                gpu_type = \"ROCM\"\n",
    "            else:\n",
    "                gpu_type = \"CUDA\"\n",
    "    elif hasattr(torch.version, 'hip') and torch.version.hip is not None:\n",
    "        gpu_type = \"ROCM\"\n",
    "\n",
    "def import_tensorflow():\n",
    "    global framework, gpu_type\n",
    "    import tensorflow as tf\n",
    "    framework = \"TENSORFLOW\"\n",
    "\n",
    "    if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "        try:\n",
    "            gpu_details = tf.config.experimental.get_device_details(tf.config.list_physical_devices('GPU')[0])\n",
    "            if 'AMD' in str(gpu_details) or 'ROCm' in str(gpu_details):\n",
    "                gpu_type = \"ROCM\"\n",
    "            else:\n",
    "                gpu_type = \"CUDA\"\n",
    "        except:\n",
    "            gpu_type = \"CUDA\"\n",
    "\n",
    "# Detect environment\n",
    "try:\n",
    "    import_pytorch()\n",
    "except ImportError:\n",
    "    try:\n",
    "        import_tensorflow()\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# Set environment variables (strings)\n",
    "os.environ['ML_FRAMEWORK'] = framework\n",
    "os.environ['GPU_TYPE'] = gpu_type\n",
    "\n",
    "# Set global boolean variables for easy checking\n",
    "pytorch = (framework == \"PYTORCH\")\n",
    "tensorflow = (framework == \"TENSORFLOW\")\n",
    "cuda = (gpu_type == \"CUDA\")\n",
    "rocm = (gpu_type == \"ROCM\")\n",
    "cpu_only = (gpu_type == \"CPU\")\n",
    "\n",
    "# Also set environment variables as strings for boolean checks\n",
    "os.environ['PYTORCH'] = str(pytorch).lower()\n",
    "os.environ['TENSORFLOW'] = str(tensorflow).lower()\n",
    "os.environ['CUDA'] = str(cuda).lower()\n",
    "os.environ['ROCM'] = str(rocm).lower()\n",
    "os.environ['CPU_ONLY'] = str(cpu_only).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d228b-1e87-4dde-a469-7e89ac27e0f3",
   "metadata": {},
   "source": [
    "## 2. Verify the installed Python version\n",
    "Multiple notebooks are available, and it can happen of system upgrades, notebooks built with different Python versions, across other possible changes.\n",
    "\n",
    "The following test will only print out the Python version installed on this notebook, so it can be verified that the expected Python is really running.\n",
    "\n",
    "> Note: this is yet a manual test, you need to know what version is supposed to run here and match with the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5964db1a-5a7c-4988-a3ee-2b7794cdfc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.11 (main, Aug 14 2025, 00:00:00) [GCC 11.5.0 20240719 (Red Hat 11.5.0-11)]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb349a7-f1f2-4a4c-8fc2-4e1a070ebe19",
   "metadata": {},
   "source": [
    "## 3. Find the version of the installed TensorFlow or PyTorch packages\n",
    "\n",
    "As both TensorFlow or PyTorch are also upgraded from time to time, it's important for us to understand which version is installed on this system, if it matches with what is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d6fefc-e681-4dd8-9350-936bf28cad72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.7.1+rocm6.3\n"
     ]
    }
   ],
   "source": [
    "if tensorflow:\n",
    "    print(f\"TensorFlow: {tf.__version__}\")\n",
    "else:\n",
    "    print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14291502-9d3b-40dc-b1a3-0721ab26e12d",
   "metadata": {},
   "source": [
    "## 3. Find the GPU on the devices list\n",
    "To understand if the GPUs are present in the current setup, we wil rely on TensorFlow Python client, which refers to the official Python API for interacting with the system's properties and devices.\n",
    "\n",
    "- If the following code returns a list with items inside, this means that there are GPUs running on this server;\n",
    "- If the following code returns an empty list, this means that there are no GPUs available;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612058a9-cbfe-4e4b-a5cc-bc08e31df830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- GPU:0 (AMD Instinct MI300X VF)\n"
     ]
    }
   ],
   "source": [
    "if tensorflow:\n",
    "    from tensorflow.python.client import device_lib\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    gpu_devices = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        gpu_devices = [f\"GPU:{i} ({torch.cuda.get_device_name(i)})\" for i in range(gpu_count)]\n",
    "\n",
    "for gpu in gpu_devices:\n",
    "    print(f\"- {gpu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f07595-a51e-4b17-b537-53d2c21ea83f",
   "metadata": {},
   "source": [
    "## 4. Check if GPU drivers (nvidia-smi or rocm-smi) loads properly\n",
    "The NVIDIA System Management Interface (nvidia-smi) or the ROCm System Management Interface (rocm-smi) are command line utilities intended to aid in the management and monitoring of NVIDIA or AMD GPU devices.\n",
    "\n",
    "The following command only spins up the `nvidia-smi` or `rocm-smi` utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4307f74-b172-4a4e-8d35-c8f6b98421f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================ ROCm System Management Interface ============================================\n",
      "====================================================== Concise Info ======================================================\n",
      "Device  Node  IDs              Temp        Power     Partitions          SCLK    MCLK    Fan  Perf  PwrCap  VRAM%  GPU%  \n",
      "\u001b[3m              (DID,     GUID)  (Junction)  (Socket)  (Mem, Compute, ID)                                                  \u001b[0m\n",
      "==========================================================================================================================\n",
      "0       2     0x74b5,   65402  40.0Â°C      141.0W    NPS1, SPX, 0        172Mhz  900Mhz  0%   auto  750.0W  0%     0%    \n",
      "==========================================================================================================================\n",
      "================================================== End of ROCm SMI Log ===================================================\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    !nvidia-smi\n",
    "elif rocm:\n",
    "    !rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed7b78-1171-4347-a99a-cedc5e513415",
   "metadata": {},
   "source": [
    "## 4. CUDA/ROCm drivers are installed\n",
    "This test aims to simply check if CUDA/ROCm drivers are properly installed. To test this, the `nvcc` command will be executed for NVIDIA GPUs and `hipcc` for ROCm GPUs.\n",
    "\n",
    "\n",
    "> Note: the code is as simple as possible, run the ones that makes sense for the tests you are doing (there are no extended programming to check automatically, etc, this is done this way on purpose to simplify the code as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a7a87d-c814-458a-b47a-36681c384223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: line 1: hipcc: command not found\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    !nvcc --version\n",
    "elif rocm:\n",
    "    !hipcc --version\n",
    "else:\n",
    "    print(\"GPU Type: None detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3ad71-70f2-4f5b-bc74-c55c075c4b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
