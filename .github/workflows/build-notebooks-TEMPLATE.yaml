# inspired by
# https://github.com/thesuperzapper/kubeflow/blob/master/.github/workflows/example_notebook_servers_publish_TEMPLATE.yaml
---
name: Build & Publish Notebook Servers (TEMPLATE)
"on":
  workflow_call:
    inputs:
      # https://docs.github.com/en/actions/learn-github-actions/variables#default-environment-variables
      # https://docs.github.com/en/actions/learn-github-actions/contexts
      target:
        required: true
        description: "make target to build"
        type: string
      python:
        required: true
        description: "python version"
        type: string
      github:
        required: true
        description: "top workflow's `github`"
        type: string
      platform:
        required: true
        description: "platform to build, podman build --platform="
        type: string
      subscription:
        required: false
        default: false
        description: "add RHEL subscription from github secret"
        type: boolean
      konflux:
        required: false
        default: false
        description: "build from Dockerfile.konflux.* instead of Dockerfile.*"
        type: boolean
      PLATFORM_RUNNERS:
        required: false
        # language=json
        default: >-
          {
            "linux/amd64": "ubuntu-24.04",
            "linux/arm64": "ubuntu-24.04-arm",
            "linux/ppc64le": "ubuntu-24.04",
            "linux/s390x": "ubuntu-24.04"
          }
        description: "Available contexts for runs-on are: github, inputs, matrix, needs, strategy, vars. Let's use inputs"
        type: string

jobs:
  build:
    # https://docs.github.com/en/actions/how-tos/using-github-hosted-runners/using-github-hosted-runners/about-github-hosted-runners#standard-github-hosted-runners-for-public-repositories
    runs-on: ${{ fromJSON(inputs.PLATFORM_RUNNERS)[inputs.platform] || 'no-such-runner' }}
    env:
      # Some pieces of code (image pulls, for example) in podman consult TMPDIR or default to /var/tmp
      TMPDIR: /home/runner/.local/share/containers/tmpdir
      # Use the rootful instance of podman for sharing images with cri-o
      # https://podman-desktop.io/blog/sharing-podman-images-with-kubernetes-cluster#introduction
      # https://access.redhat.com/solutions/6986565
      CONTAINER_HOST: unix:///var/run/podman/podman.sock
      # We don't push here when building PRs, so we can use the same IMAGE_REGISTRY in all branches of the workflow
      IMAGE_REGISTRY: "ghcr.io/${{ github.repository }}/workbench-images"
      # GitHub image registry used for storing $(CONTAINER_ENGINE)'s cache
      CACHE: "ghcr.io/${{ github.repository }}/workbench-images/build-cache"
      # https://github.com/aquasecurity/trivy
      TRIVY_VERSION: 0.68.2
      # Targets (and their folder) that should be scanned using FS instead of IMAGE scan due to resource constraints
      TRIVY_SCAN_FS_JSON: '{}'
      # Makefile variables
      BUILD_ARCH: ${{ inputs.platform }}
      RELEASE_PYTHON_VERSION: ${{ inputs.python }}

    steps:

      # image repository name must be lowercase
      - name: downcase IMAGE_REGISTRY and CACHE
        run: |
          echo "IMAGE_REGISTRY=${IMAGE_REGISTRY,,}" >>${GITHUB_ENV}
          echo "CACHE=${CACHE,,}" >>${GITHUB_ENV}

      - uses: actions/checkout@v6
        if: ${{ fromJson(inputs.github).event_name != 'pull_request_target' }}
        with:
          lfs: ${{ contains(inputs.target, 'codeserver') }}
          submodules: recursive
          persist-credentials: false  # https://github.com/actions/checkout/issues/2312
      # we need to checkout the pr branch, not pr target (the default for pull_request_target)
      # user access check is done in calling workflow
      - uses: actions/checkout@v6
        if: ${{ fromJson(inputs.github).event_name == 'pull_request_target' }}
        with:
          ref: "refs/pull/${{ fromJson(inputs.github).event.number }}/merge"
          lfs: ${{ contains(inputs.target, 'codeserver') }}
          submodules: recursive
          persist-credentials: false  # https://github.com/actions/checkout/issues/2312

      # https://github.com/docker/setup-qemu-action?tab=readme-ov-file#about
      # https://www.itix.fr/blog/qemu-user-static-with-podman/
      - name: Set up QEMU for non-native architecture
        if: ${{ contains(fromJSON('["linux/s390x", "linux/ppc64le"]'), inputs.platform) }}
        run: docker run --rm --privileged tonistiigi/binfmt --install ${platform#*/}
        env:
          platform: ${{ inputs.platform }}

      - run: mkdir -p $TMPDIR

      # do this early because it's fast and why not
      - name: Unlock encrypted secrets with git-crypt
        if: ${{ inputs.subscription }}
        run: |
          sudo apt-get update
          sudo apt-get install git-crypt
          echo "${GIT_CRYPT_KEY}" | base64 --decode > ./git-crypt-key
          git-crypt unlock ./git-crypt-key
          rm ./git-crypt-key
        env:
          GIT_CRYPT_KEY: ${{ secrets.GIT_CRYPT_KEY }}

      # https://console.redhat.com/insights/connector/activation-keys
      # This runs slower than storing the entitlement certificates with git-crypt,
      #  but on the other hand, it's then not necessary to regularly update them in the repo.
      - name: Add subscriptions from GitHub secret
        if: ${{ inputs.subscription }}
        run: |
          # https://access.redhat.com/solutions/5870841
          # https://github.com/containers/common/issues/1735
          mkdir entitlement
          mkdir consumer
          docker run \
            --platform=${{ inputs.platform }} \
            -v ${PWD}/entitlement:/etc/pki/entitlement:Z \
            -v ${PWD}/consumer:/etc/pki/consumer:Z \
            --rm -t registry.access.redhat.com/ubi9/ubi \
              /usr/sbin/subscription-manager register --org=${SUBSCRIPTION_ORG} --activationkey=${SUBSCRIPTION_ACTIVATION_KEY}
          printf "${PWD}/entitlement:/etc/pki/entitlement\n${PWD}/consumer:/etc/pki/consumer\n" | sudo tee /usr/share/containers/mounts.conf

          mkdir -p $HOME/.config/containers/

          # Don't use sudo here.
          # With CONTAINER_HOST set, podman is a remote client that forwards auth to the
          # rootful server during builds. Credentials must be readable by the runner user.
          cp ${PWD}/ci/secrets/pull-secret.json $HOME/.config/containers/auth.json
        env:
          SUBSCRIPTION_ORG: ${{ secrets.SUBSCRIPTION_ORG }}
          SUBSCRIPTION_ACTIVATION_KEY: ${{ secrets.SUBSCRIPTION_ACTIVATION_KEY }}

      # for bin/buildinputs in scripts/sandbox.py
      - uses: actions/setup-go@v6
        with:
          go-version: "stable"
          cache-dependency-path: "scripts/buildinputs/go.sum"

      - run: sudo apt-get update

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      # region Free up disk space

      - name: Free up additional disk space
        uses: ./.github/actions/free-up-disk-space
        # https://docs.github.com/en/actions/learn-github-actions/expressions
        # NOTE: the arm64 GitHub hosted runner does not have the /mnt-mounted scratch disk
        if: "${{ contains(inputs.target, 'rocm') || contains(inputs.target, 'cuda') ||
         contains(inputs.target, 'pytorch') || contains(inputs.target, 'tensorflow') ||
         contains(inputs.target, 'codeserver') || inputs.platform == 'linux/arm64' }}"

      - id: install-compsize
        run: sudo apt-get install -y btrfs-compsize

      - name: Mount lvm overlay for podman builds
        run: |
          df -h
          free -h

          bash ./ci/cached-builds/gha_lvm_overlay.sh

          df -h
          free -h

      # endregion

      # region Podman setup

      - name: Install Podman
        uses: './.github/actions/install-podman-action'
        with:
          platform: ${{ inputs.platform }}

      - name: Calculate image name and tag
        id: calculated_vars
        run: |
          # Need for sanitization explained in https://github.com/opendatahub-io/notebooks/issues/631
          # For length, Docker image tags have 128-character limit, and we form them as <inputs.target>-<ref_name>_<sha>
          # therefore since sha is 40 characters, and our target names are <40 chars, we should cut ref_name at 40
          SANITIZED_REF_NAME=$(echo "${{ github.ref_name }}" | sed 's/[^a-zA-Z0-9._-]/_/g') | cut -c 1-40
          IMAGE_TAG="${SANITIZED_REF_NAME}_${{ github.sha }}"

          echo "IMAGE_TAG=${IMAGE_TAG}" >> "$GITHUB_OUTPUT"
          echo "OUTPUT_IMAGE=${{ env.IMAGE_REGISTRY}}:${{ inputs.target }}-${IMAGE_TAG}" >> "$GITHUB_OUTPUT"

          echo "SANITIZED_PLATFORM=$(echo "${{ inputs.platform }}" | sed 's/[^a-zA-Z0-9._-]/_/g')" >> "$GITHUB_OUTPUT"

      # endregion

      # region Image build

      - name: Login to quay.io/aipcc (if the secret is present)
        # if: for the ${{ secret != '' }} does not work, getting Unrecognized named-value: 'secrets'.
        shell: bash
        run: |
          if [[ "${{ secrets.AIPCC_QUAY_BOT_USERNAME }}" == "" ]]; then
            echo "AIPCC_QUAY_BOT_USERNAME is not set, skipping login"
            exit 0
          fi
          echo "${{ secrets.AIPCC_QUAY_BOT_PASSWORD }}" | podman login quay.io/aipcc -u "${{ secrets.AIPCC_QUAY_BOT_USERNAME }}" --password-stdin

      # https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#push
      # https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#pull_request

      # We use `CONTAINER_BUILD_CACHE_ARGS` to smuggle in various misc `podman build` args, not just cache-related ones
      - name: Compute CONTAINER_BUILD_CACHE_ARGS
        id: extra-container-build-args
        shell: python
        # language=python
        run: |
          import os
          import sys
          from typing import Literal

          extra_podman_build_args = ""
          if "${{ inputs.platform }}" == "linux/s390x":
             # workaround for known issue https://github.com/zeromq/libzmq/pull/4486
             # In qemu-user, CACHELINE_SIZE probe is undefined
             # Compiling pyzmq through UV needs this in CFLAGS
             extra_podman_build_args += '--env=CFLAGS=-Dundefined=64 --env=CXXFLAGS=-Dundefined=64 --unsetenv=CFLAGS --unsetenv=CXXFLAGS'

          if "codeserver" in "${{ inputs.target }}":
             # Hermetic codeserver builds compile from source on every run — there
             # is no effective layer cache to reuse.  --layers=false stops podman
             # from persisting intermediate layers within each stage, cutting peak
             # disk use roughly in half.
             extra_podman_build_args += ' --layers=false'
             # Cap V8 heap + parallelism to fit GHA runners (4 vCPUs, 16 GB RAM).
             # These ARGs only take effect here; Konflux omits them so the
             # Dockerfile defaults apply (no limits / 16384 for MAX_OLD_SPACE_SIZE).
             # MAX_OLD_SPACE_SIZE caps the gulp VS Code build AND transpiler workers
             # (build-vscode.sh hardcodes 16384 which OOMs on 16 GB runners).
             # JOBS=1 limits transpiler workers to 1 thread — under QEMU arm64
             # emulation, 2 workers at any budget exhaust the 16 GB runner.
             # Budget: 2048 (main) + 1×4096 (worker) = ~6 GB V8 heap → ~10 GB for OS.
             # The worker (transpiler) needs more heap than the main gulp process:
             # arm64 compilation OOMs at 3072 MB (ERR_WORKER_OUT_OF_MEMORY).
             # Main process only orchestrates — 2048 MB is sufficient.
             # Uses --build-arg so the ARG values are available in intermediate stages
             # (--env only applies to the final stage in multi-stage builds).
             # ARG values don't persist in the final image (unlike ENV).
             extra_podman_build_args += ' --build-arg=NODE_OPTIONS=--max-old-space-size=1024 --build-arg=JOBS=1 --build-arg=MAX_OLD_SPACE_SIZE=2048 --build-arg=WORKER_MAX_OLD_SPACE_SIZE=4096'

          event_name: Literal['push', 'pull_request', 'pull_request_target', 'schedule', 'workflow_dispatch'] = "${{ fromJson(inputs.github).event_name }}"
          cache = "${{ env.CACHE }}"

          if event_name == "schedule":
              # For schedule events, skip --cache-from to regenerate cache
              cache_args = f"--cache-to {cache}"
              print("Regenerating cache (schedule event)", file=sys.stderr)
          elif event_name == "push" or event_name == "workflow_dispatch":
              # push or workflow_dispatch: use and update cache
              cache_args = f"--cache-from {cache} --cache-to {cache}"
              print("Using and updating cache", file=sys.stderr)
          elif event_name == "pull_request" or event_name == "pull_request_target":
              # pull_request/pull_request_target: use cache, but we don't have write access, so we can't update even if we wanted to
              cache_args = f"--cache-from {cache}"
              print("Using cache", file=sys.stderr)
          else:
              raise ValueError(f"Unexpected event name: {event_name}")

          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
             f.write(f"CONTAINER_BUILD_CACHE_ARGS={extra_podman_build_args} {cache_args}\n")

      # Hermetic builds: when the target ships a prefetch-input/ directory,
      # download all dependencies into cachi2/output/ so the Dockerfile can
      # build fully offline.  Targets without prefetch-input/ are unaffected.
      - name: "Prefetch hermetic build dependencies"
        id: prefetch
        run: |
          set -Eeuxo pipefail
          COMPONENT_DIR=$(echo "${{ inputs.target }}" | sed 's|-|/|')
          if [ -d "$COMPONENT_DIR/prefetch-input" ]; then
            echo "Hermetic build detected — prefetching dependencies for $COMPONENT_DIR"
            pip3 install --quiet --break-system-packages pyyaml
            command -v uv &>/dev/null || pip3 install --quiet --break-system-packages uv
            scripts/lockfile-generators/prefetch-all.sh --component-dir "$COMPONENT_DIR"
            if [ -x scripts/lockfile-generators/post-prefetch.sh ]; then
              scripts/lockfile-generators/post-prefetch.sh --component-dir "$COMPONENT_DIR"
            fi
            echo "EXTRA_BUILD_ARGS=--volume $(pwd)/cachi2/output:/cachi2/output:Z --build-arg LOCAL_BUILD=true" >> "$GITHUB_OUTPUT"
          else
            echo "No prefetch-input/ found for $COMPONENT_DIR — skipping"
          fi

      - name: "Build: make ${{ inputs.target }}"
        id: make-target
        run: |
          # print running stats on disk and memory occupancy
          (while true; do
            echo "=== $(date -u '+%H:%M:%S') ==="
            df -h | grep "${HOME}/.local/share/containers"
            free -h
            sleep 30
          done) &

          make ${{ inputs.target }}
        env:
          IMAGE_TAG: "${{ steps.calculated_vars.outputs.IMAGE_TAG }}"
          CONTAINER_BUILD_CACHE_ARGS: "${{ steps.extra-container-build-args.outputs.CONTAINER_BUILD_CACHE_ARGS }} ${{ steps.prefetch.outputs.EXTRA_BUILD_ARGS }}"
          # We don't have access to image registry for PRs, so disable pushing
          PUSH_IMAGES: "${{ (fromJson(inputs.github).event_name == 'pull_request' || fromJson(inputs.github).event_name == 'pull_request_target') && 'no' || 'yes' }}"
          KONFLUX: "${{ inputs.konflux && 'yes' || 'no' }}"

      - name: "Show podman images information"
        run: podman images --digests

      # endregion

      # region Pytest image tests

      # https://github.com/astral-sh/setup-uv
      - name: Install the latest version of uv
        uses: astral-sh/setup-uv@v7
        with:
          version: "latest"
          python-version: "3.14"
          enable-cache: true
          cache-dependency-glob: "uv.lock"

      - name: Check uv is installed correctly
        run: uv version

      - name: Install deps
        run: uv sync --locked

      - name: Run Testcontainers container tests (in PyTest)
        id: pytest-testcontainers
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' }}
        run: |
          set -Eeuxo pipefail
          uv run pytest --capture=fd tests/containers -m 'not openshift and not cuda and not rocm' --image="${{ steps.calculated_vars.outputs.OUTPUT_IMAGE }}"
        env:
          DOCKER_HOST: "unix:///var/run/podman/podman.sock"
          TESTCONTAINERS_DOCKER_SOCKET_OVERRIDE: "/var/run/podman/podman.sock"
          # pulling the Ryuk container from docker.io introduces CI flakiness
          TESTCONTAINERS_RYUK_DISABLED: "true"

      # endregion Pytest image tests

      # region Makefile image tests

      - name: "Check if we have tests or not"
        id: have-tests
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' }}
        run: "ci/cached-builds/has_tests.py --target ${{ inputs.target }}"

      - name: "Change pull policy to IfNotPresent"
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' }}
        run: |
          set -Eeuxo pipefail

          find . \( -name "statefulset.yaml" -o -name "pod.yaml" \) -type f -exec \
            sed -i'' 's/imagePullPolicy: Always/imagePullPolicy: IfNotPresent/g' {} \;
          git diff

      # [INFO] Running command (('make deploy9-runtimes-rocm-tensorflow-ubi9-python-3.11',), {'shell': True})
      # Deploying notebook from runtimes/rocm/tensorflow/ubi9-python-3.11/kustomize/base directory...
      # sed: can't read runtimes/rocm/tensorflow/ubi9-python-3.11/kustomize/base/kustomization.yaml: No such file or directory
      - name: "Fixup paths that prevent us from running rocm tests"
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' && steps.have-tests.outputs.tests == 'true' }}
        run: |
          set -Eeuxo pipefail

          mkdir -p runtimes/rocm
          ln -s ../rocm-tensorflow runtimes/rocm/tensorflow
          ln -s ../rocm-pytorch runtimes/rocm/pytorch

      - name: Provision K8s cluster
        id: provision-k8s
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' && steps.have-tests.outputs.tests == 'true' }}
        uses: ./.github/actions/provision-k8s

      - name: "Run image tests"
        id: makefile-tests
        # skip on s390x because we are unable to install requirements-elyra.txt that's installed by runtime image tests
        # https://raw.githubusercontent.com/opendatahub-io/elyra/refs/heads/main/etc/generic/requirements-elyra.txt
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' && steps.provision-k8s.outcome == 'success' && !contains(fromJSON('["linux/s390x"]'), inputs.platform) }}
        run: python3 ci/cached-builds/make_test.py --target ${{ inputs.target }}
        env:
          IMAGE_TAG: "${{ steps.calculated_vars.outputs.IMAGE_TAG }}"
          # for make deploy, mandatory to specify for the more exotic cases
          NOTEBOOK_TAG: "${{ inputs.target }}-${{ steps.calculated_vars.outputs.IMAGE_TAG }}"

      # endregion

      - name: Run OpenShift container tests (in PyTest)
        id: pytest-openshift
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' && steps.provision-k8s.outcome == 'success' && steps.have-tests.outputs.tests == 'true' }}
        run: |
          set -Eeuxo pipefail
          uv run pytest --capture=fd tests/containers -m 'openshift and not cuda and not rocm' --image="${{ steps.calculated_vars.outputs.OUTPUT_IMAGE }}"
        env:
          # TODO(jdanek): this Testcontainers stuff should not be necessary but currently it has to be there
          DOCKER_HOST: "unix:///var/run/podman/podman.sock"
          TESTCONTAINERS_DOCKER_SOCKET_OVERRIDE: "/var/run/podman/podman.sock"
          # pulling the Ryuk container from docker.io introduces CI flakiness
          TESTCONTAINERS_RYUK_DISABLED: "true"

      # region Trivy vulnerability scan

      - name: "pull_request|schedule: resolve target if Trivy scan should run"
        id: resolve-target
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' && (fromJson(inputs.github).event_name == 'pull_request' || fromJson(inputs.github).event_name == 'schedule') }}
        env:
          EVENT_NAME: ${{ fromJson(inputs.github).event_name }}
          HAS_TRIVY_LABEL: ${{ contains(fromJson(inputs.github).event.pull_request.labels.*.name, 'trivy-scan') }}
          FS_SCAN_FOLDER: ${{ fromJson(env.TRIVY_SCAN_FS_JSON)[inputs.target] }}
        run: |
          if [[ "$EVENT_NAME" == "pull_request" && "$HAS_TRIVY_LABEL" == "true" ]]; then
            if [[ -n "$FS_SCAN_FOLDER" ]]; then
              TARGET="$FS_SCAN_FOLDER"
              TYPE="fs"
            else
              TARGET="${{ steps.calculated_vars.outputs.OUTPUT_IMAGE }}"
              TYPE="image"
            fi
          elif [[ "$EVENT_NAME" == "schedule" ]]; then
            if [[ -n "$FS_SCAN_FOLDER" ]]; then
              TARGET="$FS_SCAN_FOLDER"
              TYPE="fs"
            else
              TARGET="${{ steps.calculated_vars.outputs.OUTPUT_IMAGE }}"
              TYPE="image"
            fi
          fi

          if [[ -n "$TARGET" ]]; then
            echo "target=$TARGET" >> $GITHUB_OUTPUT
            echo "type=$TYPE" >> $GITHUB_OUTPUT
            echo "Trivy scan will run on $TARGET ($TYPE)"

            # opendatahub-io/notebooks uses CentOS Stream which is not supported by Trivy
            # for OS package scanning. Skip OS packages and only scan language dependencies.
            # red-hat-data-services/notebooks uses UBI/RHEL which is fully supported.
            # See: https://github.com/opendatahub-io/notebooks/issues/2848
            if [[ "${{ github.repository }}" == "opendatahub-io/notebooks" ]]; then
              echo "pkg-types=library" >> $GITHUB_OUTPUT
              echo "opendatahub-io/notebooks: skipping OS package scanning (CentOS Stream unsupported by Trivy)"
            else
              echo "pkg-types=os,library" >> $GITHUB_OUTPUT
            fi
          else
            echo "Trivy scan won't run"
          fi

      - name: Run Trivy vulnerability scanner
        if: ${{ !cancelled() && steps.resolve-target.outputs.target }}
        uses: ./.github/actions/trivy-scan-action
        with:
          scan-type: ${{ steps.resolve-target.outputs.type }}
          scan-target: ${{ steps.resolve-target.outputs.target }}
          trivy-version: ${{ env.TRIVY_VERSION }}
          podman-socket: /var/run/podman/podman.sock
          pkg-types: ${{ steps.resolve-target.outputs.pkg-types }}

      # endregion

      # region check-payload for FIPS compliance

      - id: check-payload-vars
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' }}
        run: |
          echo "GOPATH=${{ github.workspace }}/go-check-payload" >> "$GITHUB_OUTPUT"
        working-directory: scripts/check-payload

      # for https://github.com/openshift/check-payload to cache the built binary
      - uses: actions/setup-go@v6
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' }}
        with:
          go-version-file: "scripts/check-payload/go.mod"
          cache-dependency-path: "scripts/check-payload/go.sum"
        env:
          GOPATH: ${{ steps.check-payload-vars.outputs.GOPATH }}

      # F0512 15:43:03.219076 21568 main.go:294] Error: exec: "oc": executable file not found in $PATH
      - name: Install oc client
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' }}
        run: |
          # Install the oc client
          curl -L https://mirror.openshift.com/pub/openshift-v4/$(uname -m)/clients/ocp/stable/openshift-client-linux.tar.gz -o /tmp/openshift-client-linux.tar.gz
          tar -xzvf /tmp/openshift-client-linux.tar.gz oc
          rm -f /tmp/openshift-client-linux.tar.gz
          sudo mv ./oc /usr/local/bin

      # perform `podman image mount` ourselves, and then follow the scenario from
      # https://github.com/openshift/check-payload/pull/154, that is because
      # `check-payload scan image --spec` insists on pulling the image, even if already present,
      #  that causes trouble when checking PRs (image not pushed) and requires `podman login` as root
      #  (we run podman as root in the GHA to reuse container storage in Kubernetes)
      # use sudo to avoid
      #  podman error (args=[image mount ghcr.io/...])
      #  (stderr=Error: cannot use command "podman image mount" with the remote podman client
      # and use --preserve-env=PATH to avoid
      #  F0512 16:31:58.425584    9911 main.go:294] Error: exec: "podman": executable file not found in $PATH
      - name: Check image with check-payload for FIPS compliance
        id: fips-check
        if: ${{ !cancelled() && steps.make-target.outcome == 'success' }}
        run: |
          set -Eeuxo pipefail
          # resolve podman under current user, not under sudo/root
          PODMAN="$(which podman)"
          # mount the image
          IMAGE_MOUNT_DIR=$(sudo "${PODMAN}" image mount "${{ steps.calculated_vars.outputs.OUTPUT_IMAGE }}")
          # run the check-payload scan
          sudo --preserve-env=PATH go tool github.com/openshift/check-payload scan local --path "${IMAGE_MOUNT_DIR}"
          # unmount the image
          sudo "${PODMAN}" image unmount --all
        working-directory: scripts/check-payload
        env:
          GOPATH: ${{ steps.check-payload-vars.outputs.GOPATH }}

      # endregion

      # region TypeScript (browser) image tests

      # https://playwright.dev/docs/ci
      # https://playwright.dev/docs/docker
      # we leave little free disk space after we mount LVM for podman storage
      # not enough to install playwright; running playwright in podman uses the space we have
      - name: Run Playwright tests
        if: ${{ !cancelled() && contains(inputs.target, 'codeserver') && steps.make-target.outcome == 'success' }}
        uses: ./.github/actions/playwright-test
        with:
          test-target: ${{ steps.calculated_vars.outputs.OUTPUT_IMAGE }}
          podman-socket: /var/run/podman/podman.sock
          upload-report: ${{ fromJson(inputs.github).event_name == 'pull_request' }}
          artifact-name: "${{ inputs.target }}_${{ steps.calculated_vars.outputs.SANITIZED_PLATFORM }}_playwright-report"

      # endregion

      - run: df -h
        if: "${{ !cancelled() }}"

      - run: sudo compsize -x "${HOME}/.local/share/containers"
        if: "${{ !cancelled() && steps.install-compsize.outcome == 'success' }}"

      # print system logs, useful when hunting for OOM kills

      - run: sudo dmesg
        if: "${{ failure() }}"

      - run: journalctl --no-pager -k
        if: "${{ failure() }}"
