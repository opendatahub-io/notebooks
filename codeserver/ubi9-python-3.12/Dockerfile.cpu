############################################################################################
# Hermetic Dockerfile for codeserver/ubi9-python-3.12
#
# RHAIENG-2846: All build steps run without network access. Every dependency
# (RPMs, npm packages, Python wheels, generic tarballs) is prefetched by cachi2
# and mounted at /cachi2/output/deps/{rpm,pip,generic}/ during the build.
#
# Multi-stage layout:
#   rpm-base  -> builds code-server RPM from prefetched source (all arches)
#   whl-cache -> installs Python wheels + exports compiled C-extension wheels (ppc64le/s390x)
#   cpu-base  -> installs OS packages + tools (oc client, micropipenv, uv)
#   codeserver -> final image (code-server + nginx + Python packages)
#   tests     -> smoke test stage
############################################################################################

#########################
# configuration args    #
#########################
ARG BASE_IMAGE

# Architecture for RPM naming (e.g. amd64, x86_64, aarch64, ppc64le, s390x).
# Pass with: --build-arg ARCH=<arch>
ARG ARCH

# [HERMETIC] Set to "true" for local testing to use cachi2/ and remove
# default repos. Omit or set to "false" for production (Konflux/Tekton).
ARG LOCAL_BUILD=false

############################################################################################
# rpm-base: Build code-server from source into an RPM (all architectures)
#
# [HERMETIC] apply-patch.sh (formerly get_code_server_rpm.sh) would git-clone code-server,
# install nvm, download Node.js, and build everything with full network access. Now the
# entire build runs offline using prefetched sources:
#   - code-server source: prefetch-input/code-server/ (git submodule)
#   - Node.js/npm: installed from prefetched RPMs
#   - npm dependencies: package-lock.json resolved URLs rewritten to file:///cachi2/...
#   - Electron, node-gyp headers, ripgrep, Playwright: from prefetched tarballs
#   - Build scripts patched to avoid any network fetches (patches/*.patch)
############################################################################################
# e.g., registry.access.redhat.com/ubi9/python-312:latest
FROM ${BASE_IMAGE} AS rpm-base

# hadolint ignore=DL3002
USER root
WORKDIR /root

ENV HOME=/root
# Build-context-relative path (for COPY sources — must NOT include /root/ prefix)
ENV CODESERVER_CONTEXT=codeserver/ubi9-python-3.12
ENV CODESERVER_SOURCE_CODE=${HOME}/${CODESERVER_CONTEXT}
ENV CODESERVER_SOURCE_PREFETCH=${CODESERVER_SOURCE_CODE}/prefetch-input/code-server

ARG ARCH
ARG BASE_IMAGE
ARG LOCAL_BUILD

ARG NODE_VERSION=22.21.1
ARG CODESERVER_VERSION=v4.106.3

# GHA runners (4 vCPUs, 16 GB) pass these via --build-arg to cap memory/parallelism.
# Konflux omits them, so defaults apply (no limits).
ARG NODE_OPTIONS
ARG JOBS
ARG MAX_OLD_SPACE_SIZE
# Worker needs more heap than the main process (arm64 compilation OOMs at 3072).
# Defaults to MAX_OLD_SPACE_SIZE when not set explicitly.
ARG WORKER_MAX_OLD_SPACE_SIZE=${MAX_OLD_SPACE_SIZE:-16384}

# [HERMETIC] Import GPG keys for prefetched RPM verification.
# CentOS key needed because libX11-devel comes from CentOS Stream repos.
RUN rpm --import /cachi2/output/deps/generic/RPM-GPG-KEY-EPEL-9
RUN rpm --import /cachi2/output/deps/generic/RPM-GPG-KEY-CentOS-Official
RUN rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release

# [HERMETIC] Configure package repos: local hermeto repos for testing, or enable nodejs:22 module for Konflux.
# Hermeto organises RPMs into per-arch sub-repos (baseos, epel, crb, ubi-*, …), each with
# its own repodata/. The generated hermeto.repo already points at the correct file:// paths.
RUN if [ "${LOCAL_BUILD}" = "true" ]; then \
        rm -f /etc/yum.repos.d/* && \
        cp /cachi2/output/deps/rpm/"$(uname -m)"/repos.d/*.repo /etc/yum.repos.d/ && \
        dnf module disable nodejs -y; \
    else \
        dnf module enable nodejs:22 -y; \
    fi;

# libxkbfile-devel = util-macros + libxkbfile (Previously built from source)
# [HERMETIC] Install nfpm (RPM packager) from prefetched RPM
RUN dnf install -y \
        nodejs nodejs-devel npm \
        jq patch libtool rsync gettext gcc-toolset-14 gcc-toolset-14-libatomic-devel \
        krb5-devel libX11-devel libxkbfile-devel \
        /cachi2/output/deps/generic/nfpm-2.44.1-1.$(uname -m).rpm && \
    dnf clean all

# There was limitation on Hermeto, it can't fetch npm packages using git/ssh protocol.
# To work around this, need to fetch some npm packages as generic artifacts and copy to npm directory.
RUN cp /cachi2/output/deps/generic/npm/* /cachi2/output/deps/npm/

# [HERMETIC] Git metadata needed by code-server's build scripts (version detection, submodules).
COPY .git /root/.git
# [HERMETIC] Rewrite script: used by setup-offline-binaries.sh to rewrite npm
# "resolved" URLs from https://registry.npmjs.org/... to file:///cachi2/...
COPY scripts/lockfile-generators/rewrite-npm-urls.sh /root/scripts/lockfile-generators/

# [HERMETIC] Copy prefetched code-server source (git submodule checked out at the target version).
COPY ${CODESERVER_CONTEXT}/prefetch-input/code-server/ ${CODESERVER_SOURCE_PREFETCH}/

# [HERMETIC] Copy offline-build patches and s390x patch into the prefetched source tree.
# These are applied by apply-patch.sh via `patch -p1`.
COPY ${CODESERVER_CONTEXT}/prefetch-input/patches/code-server-${CODESERVER_VERSION}/*.patch ${CODESERVER_SOURCE_CODE}/patches/
COPY ${CODESERVER_CONTEXT}/prefetch-input/patches/setup-offline-binaries.sh ${CODESERVER_SOURCE_CODE}/patches/
COPY ${CODESERVER_CONTEXT}/prefetch-input/patches/codeserver-offline-env.sh ${CODESERVER_SOURCE_CODE}/patches/
COPY ${CODESERVER_CONTEXT}/prefetch-input/patches/apply-patch.sh ${CODESERVER_SOURCE_CODE}/

# [HERMETIC] apply-patch.sh (formerly get_code_server_rpm.sh) was simplified: it now only
# enables gcc-toolset-14 and applies patches (nfpm is installed above). The actual
# npm ci/build/release steps are done below in separate RUN commands for better caching.
RUN cd ${CODESERVER_SOURCE_CODE} && ./apply-patch.sh

# [HERMETIC] Step 1: npm ci --offline (install all npm dependencies from local cache).
# setup-offline-binaries.sh does all offline preparation in one shot:
#   - sources codeserver-offline-env.sh (ELECTRON_SKIP_BINARY_DOWNLOAD, VSCODE_OFFLINE_CACHE, etc.)
#   - populates caches for Electron, node-gyp, Playwright, ripgrep, VSCode extensions
#   - pre-populates .build/node/ and .build/builtInExtensions/ so gulp skips network downloads
#   - rewrites package-lock.json "resolved" URLs to file:///cachi2/...
# CI=1 makes ci/dev/postinstall.sh run "npm ci" (not "npm install") in subdirs,
# so resolved URLs stay absolute (file:///cachi2/...) and lockfiles are never modified.
RUN cd ${CODESERVER_SOURCE_CODE} && \
    source ./patches/setup-offline-binaries.sh && \
    cd ${CODESERVER_SOURCE_PREFETCH} && \
    CI=1 npm ci --offline --loglevel warn

# [HERMETIC] Step 2: Build code-server TypeScript
RUN . ${CODESERVER_SOURCE_CODE}/patches/codeserver-offline-env.sh && cd ${CODESERVER_SOURCE_PREFETCH} && \
    npm run build

# [HERMETIC] Step 3: Build VS Code (the embedded editor)
# Memory budget on GHA runners (4 vCPUs, 16 GB RAM):
#   Main gulp process : MAX_OLD_SPACE_SIZE=2560        (GHA) / 16384 (Konflux)
#   Transpiler worker : WORKER_MAX_OLD_SPACE_SIZE=3584 (GHA) / 16384 (Konflux)
#   JOBS=1 on GHA → one worker thread.
#   Total V8 heap: 2560 + 3584 = 6.1 GB.
#   Main needs ~2 GB for the mangler (OOMs at 2048).
#   Worker needs >3072 MB for TS compilation (ERR_WORKER_OUT_OF_MEMORY at 3072).
#
# build-vscode.sh hardcodes --max-old-space-size=16384 which OOMs on GHA.
# Patch it to respect MAX_OLD_SPACE_SIZE (defaults to 16384 when unset).
# Worker threads don't inherit --max-old-space-size from the parent — set
# explicit resourceLimits to WORKER_MAX_OLD_SPACE_SIZE.
# JOBS overrides the transpiler worker count (default: half of CPUs).
# Unset NODE_OPTIONS so the restrictive GHA limit (1024 MB) doesn't constrain
# npm or other intermediate Node.js processes during this memory-intensive step.
RUN . ${CODESERVER_SOURCE_CODE}/patches/codeserver-offline-env.sh && cd ${CODESERVER_SOURCE_PREFETCH} && \
    sed -i "s/--max-old-space-size=16384/--max-old-space-size=\${MAX_OLD_SPACE_SIZE:-16384}/" ci/build/build-vscode.sh && \
    sed -i "s|new node_worker_threads_1.default.Worker(__filename)|new node_worker_threads_1.default.Worker(__filename, { resourceLimits: { maxOldGenerationSizeMb: ${WORKER_MAX_OLD_SPACE_SIZE:-16384} } })|" lib/vscode/build/lib/tsb/transpiler.js && \
    if [ -n "${JOBS}" ]; then sed -i "/static P = /s/=.*/= ${JOBS};/" lib/vscode/build/lib/tsb/transpiler.js; fi && \
    unset NODE_OPTIONS && \
    VERSION=${CODESERVER_VERSION/v/} npm run build:vscode

# [HERMETIC] Step 4: Create release bundle.
RUN . ${CODESERVER_SOURCE_CODE}/patches/codeserver-offline-env.sh && \
    export KEEP_MODULES=1 && cd ${CODESERVER_SOURCE_PREFETCH} && \
    npm run release

RUN . ${CODESERVER_SOURCE_CODE}/patches/codeserver-offline-env.sh && \
    export KEEP_MODULES=1 && cd ${CODESERVER_SOURCE_PREFETCH} && \
    npm run release:standalone --loglevel warn

# [HERMETIC] Step 5: Package into RPM using nfpm (installed from prefetched RPM).
RUN . ${CODESERVER_SOURCE_CODE}/patches/codeserver-offline-env.sh && cd ${CODESERVER_SOURCE_PREFETCH} && \
    VERSION=${CODESERVER_VERSION/v/} npm run package && \
    ls -alh release-packages/ && \
    mv release-packages/code-server-${CODESERVER_VERSION/v/}-*.rpm /tmp/

# Free disk: only the RPM in /tmp/ is needed by downstream stages.
# The source tree, node_modules, and build artifacts (~10-15 GB) are no longer needed.
RUN rm -rf /root/.git /root/codeserver /root/scripts && \
    (dnf remove -y gcc-toolset-14 nodejs-devel npm 2>/dev/null || true) && \
    dnf clean all && rm -rf /var/cache/dnf

# Sentinel file: downstream stages use COPY --from=rpm-base /tmp/control to wait for this stage.
RUN echo "done" > /tmp/control

############################################################################################
# whl-cache: Build/cache Python wheels for all architectures
#
# [HERMETIC] On x86_64/aarch64, pre-built wheels exist on PyPI and are prefetched by cachi2.
# On ppc64le/s390x, all native Python packages (numpy, scipy, pyarrow, onnx, etc.) use
# pre-built RHOAI wheels (prefetched by cachi2 from requirements-rhoai.txt into
# /cachi2/output/deps/pip/). OpenBLAS (runtime lib for numpy/scipy) is installed via RPM.
#
# Previously this stage used `--mount=type=cache,target=/root/.cache/uv` with network
# access and `pip install uv` from PyPI. Now it uses cachi2 for hermetic builds.
############################################################################################
FROM registry.access.redhat.com/ubi9/python-312:latest AS whl-cache

# hadolint ignore=DL3002
USER root
WORKDIR /root

ENV HOME=/root

ARG LOCAL_BUILD
ARG CODESERVER_SOURCE_CODE=codeserver/ubi9-python-3.12
ARG PYLOCK_FLAVOR

# [HERMETIC] Import GPG keys for EPEL, Red Hat, and CentOS repos (needed for dnf to verify prefetched RPMs).
# EPEL + CentOS keys are prefetched as generic artifacts (see artifacts.in.yaml).
# UBI9 images only ship the Red Hat key; CentOS key is needed for ppc64le/s390x packages
# from CentOS Stream repos (mesa-libGL, etc.).
RUN rpm --import /cachi2/output/deps/generic/RPM-GPG-KEY-EPEL-9
RUN rpm --import /cachi2/output/deps/generic/RPM-GPG-KEY-CentOS-Official
RUN rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release

# [HERMETIC] Configure package repos: local hermeto repos for testing, or enable nodejs:22 module for Konflux.
# Hermeto organises RPMs into per-arch sub-repos (baseos, epel, crb, ubi-*, …), each with
# its own repodata/. The generated hermeto.repo already points at the correct file:// paths.
RUN if [ "${LOCAL_BUILD}" = "true" ]; then \
        rm -f /etc/yum.repos.d/* && \
        cp /cachi2/output/deps/rpm/"$(uname -m)"/repos.d/*.repo /etc/yum.repos.d/ && \
        dnf module disable nodejs -y; \
    fi;
# [HERMETIC] ppc64le/s390x: install system-level deps.
# openblas-threads: provides libopenblasp.so.0 (pthreads variant) that RHOAI numpy/scipy
# wheels link against at runtime. The base 'openblas' meta-package only pulls in
# openblas-serial (libopenblas.so.0) which has the wrong soname.
# gcc/gcc-c++: needed for packages with simple C extensions that only provide sdist
# on PyPI and are not available on RHOAI (e.g. psutil, tornado, greenlet).
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
if [[ $(uname -m) == "ppc64le" ]] || [[ $(uname -m) == "s390x" ]]; then
    dnf install -y gcc gcc-c++ openblas-threads \
        python3.12-devel python3-devel \
        tar mesa-libGL libxcrypt-compat
fi
EOF

# copy requirements and scripts
COPY ${CODESERVER_SOURCE_CODE}/uv.lock.d/pylock.${PYLOCK_FLAVOR}.toml ./pylock.toml

# [HERMETIC] requirements.<flavor>.txt is generated by create-requirements-lockfile.sh with pinned hashes.
# Previously we used pylock.toml directly; now cachi2 needs requirements.txt format.
COPY ${CODESERVER_SOURCE_CODE}/requirements.${PYLOCK_FLAVOR}.txt ./requirements.txt
COPY ${CODESERVER_SOURCE_CODE}/devel_env_setup.sh ./

# [HERMETIC] Install uv + install all Python packages from cachi2 pip cache.
# /cachi2/output/deps/pip/ contains PyPI wheels + RHOAI wheels (prefetched by cachi2
# from requirements.txt and requirements-rhoai.txt respectively).
RUN /bin/bash <<'EOF'
    set -Eeuxo pipefail
    pip install --no-index --find-links /cachi2/output/deps/pip uv
    echo "installed uv from cachi2"
    source ./devel_env_setup.sh
    # Install all packages from the offline cachi2 pip cache.
    # /cachi2/output/deps/pip/ contains PyPI wheels + RHOAI wheels (from requirements-rhoai.txt)
    UV_NO_CACHE=false UV_LINK_MODE=copy uv pip install --no-index \
        --find-links /cachi2/output/deps/pip \
        --strict --no-deps --refresh --no-config --no-progress --verify-hashes --compile-bytecode \
        --index-strategy=unsafe-best-match --requirements=./requirements.txt

    # [HERMETIC] Export all installed wheels to /wheelsdir/ for the codeserver stage.
    # RHOAI provides pre-built wheels for major native packages (numpy, scipy,
    # pyarrow, pyzmq, pillow, etc.), but some smaller PyPI packages (e.g. psutil,
    # greenlet, wrapt, uvloop, watchfiles) only ship sdist for ppc64le/s390x.
    # Those sdist packages are compiled here (whl-cache has gcc) and exported as
    # wheels, since the codeserver stage lacks gcc/build tools.
    if [[ $(uname -m) == "ppc64le" ]] || [[ $(uname -m) == "s390x" ]]; then
        echo "Exporting all wheels to /wheelsdir/ for codeserver stage..."
        pip freeze > /tmp/installed-packages.txt
        pip wheel --no-index \
            --find-links /cachi2/output/deps/pip \
            --find-links /wheelsdir/ \
            --no-deps \
            --wheel-dir /wheelsdir/ \
            -r /tmp/installed-packages.txt
    fi
EOF

# Sentinel file: downstream stages use COPY --from=whl-cache /tmp/control to wait for this stage.
RUN touch /tmp/control

############################################################################################
# cpu-base: Runtime base image with OS packages, tools (oc, micropipenv, uv)
#
# [HERMETIC] Previously this stage ran `subscription-manager refresh`, `dnf upgrade --refresh`,
# and `curl` to download the oc client. Now all packages come from prefetched RPMs and
# the oc client is extracted from a prefetched tarball.
############################################################################################
FROM ${BASE_IMAGE} AS cpu-base

# [HERMETIC] Needed for conditional repo setup (same pattern as rpm-base stage).
ARG LOCAL_BUILD

WORKDIR /opt/app-root/bin

# OS Packages needs to be installed as root
USER 0

# [HERMETIC] Import GPG keys for prefetched RPM verification.
# CentOS key needed because mesa-libGL comes from CentOS Stream repos.
RUN rpm --import /cachi2/output/deps/generic/RPM-GPG-KEY-EPEL-9
RUN rpm --import /cachi2/output/deps/generic/RPM-GPG-KEY-CentOS-Official
RUN rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release

# [HERMETIC] Configure package repos: local hermeto repos for testing, or enable nodejs:22 module for Konflux.
# Hermeto organises RPMs into per-arch sub-repos (baseos, epel, crb, ubi-*, …), each with
# its own repodata/. The generated hermeto.repo already points at the correct file:// paths.
RUN if [ "${LOCAL_BUILD}" = "true" ]; then \
        rm -f /etc/yum.repos.d/* && \
        cp /cachi2/output/deps/rpm/"$(uname -m)"/repos.d/*.repo /etc/yum.repos.d/ && \
        dnf module disable nodejs -y; \
    fi;

# [HERMETIC] Install Node.js runtime from prefetched RPMs.
RUN dnf install -y nodejs npm && dnf clean all

# Install useful OS packages (runtime dependencies)
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
dnf install -y tar perl mesa-libGL skopeo
dnf clean all
rm -rf /var/cache/dnf
EOF

# ppc64le/s390x: install openblas-threads (provides libopenblasp.so.0 at runtime for numpy/scipy)
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
if [[ $(uname -m) == "ppc64le" ]] || [[ $(uname -m) == "s390x" ]]; then
    dnf install -y openblas-threads
    dnf clean all
fi
EOF

# Other apps and tools installed as default user
USER 1001

### [HERMETIC] Install micropipenv and uv from cachi2 pip cache (offline).
### Previously: pip install --extra-index-url https://pypi.org/simple
RUN pip install --no-cache-dir --no-index --find-links /cachi2/output/deps/pip "micropipenv[toml]==1.10.0" "uv==0.9.28"

### [HERMETIC] Install the oc client from prefetched tarball (see artifacts.in.yaml).
### Previously: curl from mirror.openshift.com at build time.
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
ARCH=$(uname -m)
tar -xzvf /cachi2/output/deps/generic/openshift-client-linux-${ARCH}.tar.gz oc
EOF

####################
# codeserver       #
####################
FROM cpu-base AS codeserver

ARG TARGETOS
ARG TARGETARCH

ARG CODESERVER_SOURCE_CODE=codeserver/ubi9-python-3.12
ARG CODESERVER_VERSION=v4.106.3
ARG PYLOCK_FLAVOR

LABEL name="odh-notebook-code-server-ubi9-python-3.12" \
      summary="code-server image with python 3.12 based on UBI 9" \
      description="code-server image with python 3.12 based on UBI9" \
      io.k8s.display-name="code-server image with python 3.12 based on UBI9" \
      io.k8s.description="code-server image with python 3.12 based on UBI9" \
      authoritative-source-url="https://github.com/opendatahub-io/notebooks" \
      io.openshift.build.commit.ref="main" \
      io.openshift.build.source-location="https://github.com/opendatahub-io/notebooks/tree/main/codeserver/ubi9-python-3.12" \
      io.openshift.build.image="quay.io/opendatahub/workbench-images:codeserver-ubi9-python-3.12"

USER 0

WORKDIR /opt/app-root/bin

# [HERMETIC] Configure package repos: local hermeto repos for testing, or enable nodejs:22 module for Konflux.
# Hermeto organises RPMs into per-arch sub-repos (baseos, epel, crb, ubi-*, …), each with
# its own repodata/. The generated hermeto.repo already points at the correct file:// paths.
RUN if [ "${LOCAL_BUILD}" = "true" ]; then \
        rm -f /etc/yum.repos.d/* && \
        cp /cachi2/output/deps/rpm/"$(uname -m)"/repos.d/*.repo /etc/yum.repos.d/ && \
        dnf module disable nodejs -y; \
    else \
        dnf module enable nodejs:22 -y; \
    fi;

# [HERMETIC] Install useful OS packages from prefetched RPMs.
# nodejs: provides libnode.so needed at runtime by code-server's bundled node binary
#         (installed via rpm2cpio which skips dependency resolution).
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
PACKAGES=(
    nodejs
    jq git-lfs libsndfile
    # provides envsubst which is required by run-nginx.sh
    gettext
    # patchelf binary: kept for safety in case any remaining packages use
    # meson-python build backend and need to build from sdist.
    patchelf
)
dnf install -y "${PACKAGES[@]}"
dnf clean all
rm -rf /var/cache/dnf
EOF

# Wait for rpm-base stage (builds code-server RPM from source).
COPY --from=rpm-base /tmp/control /dev/null

# [HERMETIC] Copy the built RPM from rpm-base stage.
# Previously this used --mount=type=cache,from=rpm-base but bind mounts fail on Konflux.
COPY --from=rpm-base /tmp/code-server-*.rpm /tmp/

# Install code-server
# Note: Use COPY instead of bind mounts; bind mounts fail on Konflux.
# https://redhat-internal.slack.com/archives/C04PZ7H0VA8/p1755628065772589?thread_ts=1755597929.335999&cid=C04PZ7H0VA8
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
# EXPLANATION: dnf installation produces an "unsigned rpm" error from Konflux (Conforma)
#  since we're building rpm from source, we will simply unpack it over /
# dnf install -y "/code-server-rpm/code-server-${CODESERVER_VERSION/v/}-${TARGETARCH}.rpm"
# dnf -y clean all --enablerepo='*'
dnf install -y cpio
dnf -y clean all
cd /
ls /tmp/
rpm2cpio "/tmp/code-server-${CODESERVER_VERSION/v/}-${TARGETARCH}.rpm" | cpio -idmv
EOF

COPY --chown=1001:0 ${CODESERVER_SOURCE_CODE}/utils utils/

# Create and intall the extensions though build-time on a temporary directory. Later this directory will copied on the `/opt/app-root/src/.local/share/code-server/extensions` via run-code-server.sh file when it starts up.
# https://coder.com/docs/code-server/FAQ#how-do-i-install-an-extension
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
mkdir -p /opt/app-root/extensions-temp
code-server --install-extension /opt/app-root/bin/utils/ms-python.python-2026.0.0.vsix --extensions-dir /opt/app-root/extensions-temp
code-server --install-extension /opt/app-root/bin/utils/ms-toolsai.jupyter-2025.9.1.vsix --extensions-dir /opt/app-root/extensions-temp
EOF

# Install NGINX to proxy code-server and pass probes check
ENV APP_ROOT=/opt/app-root

ENV NGINX_VERSION=1.24 \
    NGINX_SHORT_VER=124 \
    NGINX_CONF_PATH=/etc/nginx/nginx.conf \
    NGINX_CONTAINER_SCRIPTS_PATH=/usr/share/container-scripts/nginx \
    NGINX_LOG_PATH=/var/log/nginx

ENV NGINX_CONFIGURATION_PATH=${APP_ROOT}/etc/nginx.d \
    NGINX_DEFAULT_CONF_PATH=${APP_ROOT}/etc/nginx.default.d \
    NGINX_APP_ROOT=${APP_ROOT} \
    NGINX_PERL_MODULE_PATH=${APP_ROOT}/etc/perl

# [HERMETIC] Install nginx/httpd from prefetched RPMs.
# NOTE: httpd is prefetched from UBI repos (not CentOS Stream) via the
# exclude=httpd* rule in centos.repo, ensuring the prefetched version matches
# the httpd already in the UBI9 base image and avoids httpd-devel conflicts.
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
INSTALL_PKGS="bind-utils nginx nginx-mod-stream nginx-mod-http-perl httpd"
dnf install -y --setopt=tsflags=nodocs $INSTALL_PKGS
rpm -V $INSTALL_PKGS
dnf -y clean all --enablerepo='*'
EOF

# Configure httpd for CGI processing
COPY --chown=1001:0 ${CODESERVER_SOURCE_CODE}/httpd/httpd.conf /etc/httpd/conf/httpd.conf
COPY --chown=1001:0 ${CODESERVER_SOURCE_CODE}/httpd/codeserver-cgi.conf /etc/httpd/conf.d/codeserver-cgi.conf

# Copy extra files to the image.
COPY --chown=1001:0 ${CODESERVER_SOURCE_CODE}/nginx/root/ /

## Configure nginx
COPY ${CODESERVER_SOURCE_CODE}/nginx/serverconf/ /opt/app-root/etc/nginx.default.d/
COPY ${CODESERVER_SOURCE_CODE}/nginx/httpconf/ /opt/app-root/etc/nginx.d/
COPY ${CODESERVER_SOURCE_CODE}/nginx/api/ /opt/app-root/api/

# Changing ownership and user rights to support following use-cases:
# 1) running container on OpenShift, whose default security model
#    is to run the container under random UID, but GID=0
# 2) for working root-less container with UID=1001, which does not have
#    to have GID=0
# 3) for default use-case, that is running container directly on operating system,
#    with default UID and GID (1001:0)
# Supported combinations of UID:GID are thus following:
# UID=1001 && GID=0
# UID=<any>&& GID=0
# UID=1001 && GID=<any>
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
sed -i -f ${NGINX_APP_ROOT}/nginxconf.sed ${NGINX_CONF_PATH}
mkdir -p ${NGINX_APP_ROOT}/etc/nginx.d/
mkdir -p ${NGINX_APP_ROOT}/etc/nginx.default.d/
mkdir -p ${NGINX_APP_ROOT}/api/
mkdir -p ${NGINX_CONTAINER_SCRIPTS_PATH}/nginx-start
mkdir -p ${NGINX_LOG_PATH}
mkdir -p ${NGINX_PERL_MODULE_PATH}
# Create httpd directories and set permissions
mkdir -p /var/log/httpd /var/run/httpd /etc/httpd/logs
chown -R 1001:0 ${NGINX_CONF_PATH}
chown -R 1001:0 ${NGINX_APP_ROOT}/etc
chown -R 1001:0 ${NGINX_CONTAINER_SCRIPTS_PATH}/nginx-start
chown -R 1001:0 /var/lib/nginx /var/log/nginx /run
chown -R 1001:0 /var/log/httpd /var/run/httpd /etc/httpd/logs
chmod    ug+rw  ${NGINX_CONF_PATH}
chmod -R ug+rwX ${NGINX_APP_ROOT}/etc
chmod -R ug+rwX ${NGINX_CONTAINER_SCRIPTS_PATH}/nginx-start
chmod -R ug+rwX /var/lib/nginx /var/log/nginx /run
chmod -R ug+rwX /var/log/httpd /var/run/httpd /etc/httpd/logs
# Make CGI script executable
chmod +x /opt/app-root/api/kernels/access.cgi
rpm-file-permissions
# Ensure the temporary directory and target directory have the correct permissions
mkdir -p /opt/app-root/src/.local/share/code-server/extensions
mkdir -p /opt/app-root/src/.local/share/code-server/coder-logs
mkdir -p /opt/app-root/src/.config/code-server
chown -R 1001:0 /opt/app-root/src/.local/share/code-server
chown -R 1001:0 /opt/app-root/extensions-temp
chown -R 1001:0 /opt/app-root/src/.config/code-server
EOF

# Launcher
COPY --chown=1001:0 ${CODESERVER_SOURCE_CODE}/run-code-server.sh ${CODESERVER_SOURCE_CODE}/run-nginx.sh ./

ENV SHELL=/bin/bash

ENV PYTHONPATH=/opt/app-root/bin/python3

# Install useful packages from pylock file
COPY ${CODESERVER_SOURCE_CODE}/uv.lock.d/pylock.${PYLOCK_FLAVOR}.toml ./pylock.toml

# [HERMETIC] Install Python packages from cachi2 pip cache (requirements.<flavor>.txt format).
# Previously used pylock.toml; cachi2 prefetch generates requirements.txt with hashes.
COPY ${CODESERVER_SOURCE_CODE}/requirements.${PYLOCK_FLAVOR}.txt ./requirements.txt

# Wait for whl-cache stage (builds/caches Python wheels).
COPY --from=whl-cache /tmp/control /dev/null

# [HERMETIC] Install all Python packages from cachi2 pip cache (--no-index).
# /cachi2/output/deps/pip/ contains both PyPI wheels and RHOAI ppc64le/s390x wheels
# (prefetched by cachi2 from requirements.txt and requirements-rhoai.txt).
# /wheelsdir/ from whl-cache contains compiled wheels for PyPI packages that only
# ship sdist for ppc64le/s390x (e.g. psutil, greenlet, wrapt, uvloop, watchfiles).
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=cache,from=whl-cache,source=/wheelsdir/,target=/wheelsdir/,rw /bin/bash <<'EOF'
set -Eeuxo pipefail
echo "Installing softwares and packages"
# Install all requirements from the offline cachi2 pip cache.
# --find-links order:
#   1. /wheelsdir/ : compiled C-extension wheels from whl-cache stage (ppc64le/s390x)
#   2. /cachi2/output/deps/pip/ : PyPI wheels + RHOAI wheels (from requirements-rhoai.txt)
#
# --no-verify-hashes: requirements.txt contains --hash=sha256:… lines for the
# original PyPI/RHOAI files.  Packages compiled from sdist in the whl-cache
# stage (e.g. psutil, greenlet) produce wheels with different hashes.
# Hash verification already passed in whl-cache; skip it here so uv accepts
# the locally compiled wheels from /wheelsdir/.
UV_NO_CACHE=false UV_LINK_MODE=copy uv pip install --no-index \
    --no-verify-hashes \
    --find-links /wheelsdir/ \
    --find-links /cachi2/output/deps/pip \
    --cache-dir /root/.cache/uv \
    --requirements=./requirements.txt

# change ownership to default user (all packages were installed as root and has root:root ownership
chown -R 1001:0 /opt/app-root
EOF

USER 1001

# Fix permissions to support pip in Openshift environments
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
chmod -R g+w /opt/app-root/lib/python3.12/site-packages
fix-permissions /opt/app-root -P
EOF

WORKDIR /opt/app-root/src

CMD ["/opt/app-root/bin/run-code-server.sh"]

# [HERMETIC] Smoke test stage.
FROM codeserver as tests
ARG CODESERVER_SOURCE_CODE=codeserver/ubi9-python-3.12
COPY ${CODESERVER_SOURCE_CODE}/test /tmp/test
# TODO(jdanek): add --mount=type=bind,target=/opt/app-root/src
RUN /bin/bash <<'EOF'
set -Eeuxo pipefail
python3 /tmp/test/test_startup.py 2>&1 | tee /tmp/test_log.txt
EOF

FROM codeserver
COPY --from=tests /tmp/test_log.txt /tmp/test_log.txt
